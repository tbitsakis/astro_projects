{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Galaxy classification\n",
    "This is a ML code that performs galaxy classification using user-defined labels from the Galaxy-Zoo (http://zoo1.galaxyzoo.org/) project. \n",
    "\n",
    "The steps are the following: <br>\n",
    "1. Import libraries and sample\n",
    "2. Split sub-samples (training, validation, testing)\n",
    "3. Fit with various classifier and check performance\n",
    "4. Compare various classifiers in testing sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, svm, utils, metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAABCCAYAAAB3uzx2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAABW1JREFUeJzt3bFSFFkUh/Fha3OQF0B8ARDIkSqNMcEUTCSFCDMg00xCIiCFBGKpQnIs4QEUeQGQJ2CDjc5Xa9/p3jtDnd3vl52C6bkzt/vU1L/v3Bl5eHjoSZLy+OOxByBJasfGLUnJ2LglKRkbtyQlY+OWpGRs3JKUjI1bkpKxcUtSMjZuSUrmzwEdt/HrmEdHR6He2NgI9atXr0L94cOHUD958qT0/CNtx0QvXrwI9a9fv0K9vb0d6sXFxS5jaj2uL1++hPr169ehnp6ebvz/WuP6+PFjqN+/fx/qycnJUH/9+jXUHeewOC7ivK2srIT6+Pi4zeF6vY7j4vn09OnTUO/v77cdB1V5v0rn/eXlZZvD9Xodx/Xp06fGcXDerq6uQj06Ohrqnz9/hnpsbKzTuNbW1hrHwfOL/z82NtZ0+F7v9+9X4CduSUrGxi1Jydi4JSmZQWXcjZhpX19fh/ru7i7U4+PjoT48PAz10tJSxdH9jVnU+fl5qM/OzkLdR8bdCTPFhYWFUJeyvFqYYXMOdnd3Q726uhpqZtwvX76sOLrfY3bMewDDwnnh+XRwcBDqiYmJxsfXcnJy0jiuzc3NgTxvW7wemYGXMvE+suW+lDJ+nm+8x9THPae++IlbkpKxcUtSMjZuSUpmKBk3801m2t+/fw/1s2fPQs113TxejYyb2VUpixpWVsp1olNTU6HmOm6uL6/l3bt3oeZ9itnZ2VBzHfewMm1mm8wcua62lB1zvXVXzFhvbm5CzXsVpfXUtTLbUobN82tYOE+0tbUVas5jrSyZeN2X1uNznjguznO//MQtScnYuCUpGRu3JCUzlIyb67JnZmZCzUybmJ/WwHWfzMzu7+8bH981m2qLWR8zNf59UOvJOUc/fvwINe9bMNPmOdDHXiWdMGNk9tl2LwmeF11x3ri3Bs83Zqm1Mm1ids57KMO6l9N2vTOvXyrtIdIVj/P8+fNQ/8OeKKGudc/ET9ySlIyNW5KSsXFLUjKPknFzXXbbx9fIR5ltMrsqPQezwVp4XGZ5pf2jK+zr3Bdm3re3t6Fmxs369PQ01F3nlHttrK+vh3p5ebnx8Ts7O6He29vrNI4SzhszXH6PgK+DSuuc+8XzjRkszz+u666V2fI4bb9Xwfd3UPegStc993rhvR8zbkn6n7JxS1IyNm5JSmYoGTfzS+41Qsy0Ly4uQv3mzZs6A/sXmMHVWu/KdcPMYInZ3qDW+5Zwjplhc39u/mYlf1e0X9zjgzX3uS7tpzysvTnaZrCD2o+bmSszWma6zN6/ffsW6q7XAcfB83pkZKTx74PKtEv74XOvF84TzyeOu2vm7SduSUrGxi1Jydi4JSmZoWTcXPPLzPro6KixJu4F/V/C9eRcv8o9Lpihca+St2/fNv69K/4GZWlvks+fP4e61n2K0r7VzCj5/1znPah7BFxvziy+tCfKoLJ3nm/MsJnBMsNlZlvrXg/XqfP9mp+fr/I8JXz9HEdpf3fuZcLvWXTdC8dP3JKUjI1bkpKxcUtSMo+ScXMNLzPrubm5UJfWfdfAbJNZMDNKZs+19vtlRsiMljUzMo6TGV2tjJvrtvmblMRMe3d3t8o4Sjiv3Pe61ryVnJ2dhbq0Pp/Z+6DWKfP1M6NlJstxDCp75/XF9fjD+r4Cn4evn9cBM3Beb7X2mPETtyQlY+OWpGRs3JKUzMjDw8Njj0GS1IKfuCUpGRu3JCVj45akZGzckpSMjVuSkrFxS1IyNm5JSsbGLUnJ2LglKRkbtyQlY+OWpGRs3JKUjI1bkpKxcUtSMjZuSUrGxi1Jydi4JSkZG7ckJWPjlqRkbNySlIyNW5KSsXFLUjI2bklK5i9LQentGXSYUwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In this part we are going to upload the galaxy images. \n",
    "# For now I use a dataset of digits from scikit-learn\n",
    "# I plot some examples\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "for index, (image, label) in enumerate(images_and_labels[:9]):\n",
    "    plt.subplot(2, 9, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "#     plt.title('Training: %i' % label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are going to use astropy to read fits images and convert them into \n",
    "# similar format matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Using scikit-learn / SVM classifier\n",
    "We are going to test the performance of a classical Support Vector Machines classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To apply a classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "labels = digits.target\n",
    "\n",
    "# Splitting in training, validation, and test samples\n",
    "n_samples = len(digits.images)\n",
    "\n",
    "data_train = data[:8 * n_samples // 10] # i.e. 80% training\n",
    "labels_train = labels[:8 * n_samples // 10]\n",
    "\n",
    "data_valid = data[8 * n_samples // 10:9 * n_samples // 10] # i.e. 10% validation (80->90%)\n",
    "labels_valid = labels[8 * n_samples // 10:9 * n_samples // 10]\n",
    "\n",
    "data_test = data[9 * n_samples // 10:] # i.e. 10% testing (90->100%)\n",
    "labels_test = labels[9 * n_samples // 10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False):\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.97        19\n",
      "          1       0.94      1.00      0.97        17\n",
      "          2       1.00      1.00      1.00        18\n",
      "          3       1.00      0.89      0.94        19\n",
      "          4       0.94      0.94      0.94        17\n",
      "          5       1.00      1.00      1.00        19\n",
      "          6       1.00      1.00      1.00        19\n",
      "          7       0.94      1.00      0.97        17\n",
      "          8       0.93      0.88      0.90        16\n",
      "          9       0.90      1.00      0.95        19\n",
      "\n",
      "avg / total       0.97      0.97      0.97       180\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classifier\n",
    "model_svc = SVC(gamma=0.001)\n",
    "model_svc.fit(data_train, labels_train)\n",
    "\n",
    "# Comparisong with prediction\n",
    "predicted = model_svc.predict(data_valid)\n",
    "\n",
    "print(\"Classification report for %s:\\n%s\\n\"\n",
    "      % (model_svc, metrics.classification_report(labels_valid, predicted)))\n",
    "# print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(labels_valid, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Using scikit-learn / Multilayer perceptron classifier\n",
    "We are going to use a simple Neural Network (Multilayer perceptron) classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False):\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.97        19\n",
      "          1       0.89      0.94      0.91        17\n",
      "          2       1.00      1.00      1.00        18\n",
      "          3       1.00      0.84      0.91        19\n",
      "          4       0.89      0.94      0.91        17\n",
      "          5       0.90      1.00      0.95        19\n",
      "          6       1.00      0.95      0.97        19\n",
      "          7       0.84      0.94      0.89        17\n",
      "          8       0.73      0.69      0.71        16\n",
      "          9       0.95      0.95      0.95        19\n",
      "\n",
      "avg / total       0.92      0.92      0.92       180\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classifier\n",
    "model_MLP = MLPClassifier()\n",
    "model_MLP.fit(data_train, labels_train)\n",
    "\n",
    "# Comparisong with prediction\n",
    "predicted = model_MLP.predict(data_valid)\n",
    "\n",
    "print(\"Classification report for %s:\\n%s\\n\"\n",
    "      % (model_MLP, metrics.classification_report(labels_valid, predicted)))\n",
    "# print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(labels_valid, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Using scikit learn / Random Forests\n",
    "We are using a a scikit-learn bagging classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False):\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.89      0.89        19\n",
      "          1       0.76      0.76      0.76        17\n",
      "          2       0.89      0.94      0.92        18\n",
      "          3       0.94      0.89      0.92        19\n",
      "          4       0.80      0.94      0.86        17\n",
      "          5       1.00      0.89      0.94        19\n",
      "          6       0.95      0.95      0.95        19\n",
      "          7       0.83      0.88      0.86        17\n",
      "          8       0.86      0.75      0.80        16\n",
      "          9       0.79      0.79      0.79        19\n",
      "\n",
      "avg / total       0.88      0.87      0.87       180\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classifier\n",
    "model_RF = RandomForestClassifier()\n",
    "model_RF.fit(data_train, labels_train)\n",
    "\n",
    "# Comparisong with prediction\n",
    "predicted = model_RF.predict(data_valid)\n",
    "\n",
    "print(\"Classification report for %s:\\n%s\\n\"\n",
    "      % (model_RF, metrics.classification_report(labels_valid, predicted)))\n",
    "# print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(labels_valid, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Using Keras simple Neural Network classifier\n",
    "We are using a simple multilayer perceptron classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing data to 0-1 range\n",
    "data = data / np.max(data[:])\n",
    "\n",
    "num_pixels = data_train.shape[1] \n",
    "\n",
    "# Since the output variable is an integer 0-9. \n",
    "# This is a multi-class classification problem. \n",
    "# It is good practice to use a one hot encoding \n",
    "# of the class values, transforming the vector \n",
    "# of class integers into a binary matrix.\n",
    "uniques, labels_valid = np.unique(labels_valid, return_inverse=True)\n",
    "labels_train_cat = np_utils.to_categorical(labels_train)\n",
    "labels_valid_cat = np_utils.to_categorical(labels_valid)\n",
    "num_classes = labels_valid_cat.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model_Ker = Sequential()\n",
    "    model_Ker.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', \n",
    "                                                                activation='relu'))\n",
    "    model_Ker.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model_Ker.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model_Ker\n",
    "\n",
    "# Build the model\n",
    "model_Ker = baseline_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1437 samples, validate on 180 samples\n",
      "Epoch 1/30\n",
      " - 0s - loss: 1.8855 - acc: 0.4168 - val_loss: 1.5032 - val_acc: 0.6444\n",
      "Epoch 2/30\n",
      " - 0s - loss: 1.2161 - acc: 0.7773 - val_loss: 1.1140 - val_acc: 0.7278\n",
      "Epoch 3/30\n",
      " - 0s - loss: 0.8218 - acc: 0.8594 - val_loss: 0.8633 - val_acc: 0.7556\n",
      "Epoch 4/30\n",
      " - 0s - loss: 0.6074 - acc: 0.8873 - val_loss: 0.7509 - val_acc: 0.7778\n",
      "Epoch 5/30\n",
      " - 0s - loss: 0.4752 - acc: 0.9137 - val_loss: 0.6758 - val_acc: 0.8056\n",
      "Epoch 6/30\n",
      " - 0s - loss: 0.3897 - acc: 0.9325 - val_loss: 0.5996 - val_acc: 0.8278\n",
      "Epoch 7/30\n",
      " - 0s - loss: 0.3331 - acc: 0.9339 - val_loss: 0.5767 - val_acc: 0.8333\n",
      "Epoch 8/30\n",
      " - 0s - loss: 0.2954 - acc: 0.9429 - val_loss: 0.5337 - val_acc: 0.8333\n",
      "Epoch 9/30\n",
      " - 0s - loss: 0.2600 - acc: 0.9450 - val_loss: 0.5346 - val_acc: 0.8333\n",
      "Epoch 10/30\n",
      " - 0s - loss: 0.2366 - acc: 0.9548 - val_loss: 0.5191 - val_acc: 0.8278\n",
      "Epoch 11/30\n",
      " - 0s - loss: 0.2175 - acc: 0.9555 - val_loss: 0.4837 - val_acc: 0.8444\n",
      "Epoch 12/30\n",
      " - 0s - loss: 0.1994 - acc: 0.9576 - val_loss: 0.4767 - val_acc: 0.8444\n",
      "Epoch 13/30\n",
      " - 0s - loss: 0.1892 - acc: 0.9596 - val_loss: 0.4734 - val_acc: 0.8333\n",
      "Epoch 14/30\n",
      " - 0s - loss: 0.1752 - acc: 0.9631 - val_loss: 0.4441 - val_acc: 0.8611\n",
      "Epoch 15/30\n",
      " - 0s - loss: 0.1643 - acc: 0.9631 - val_loss: 0.4514 - val_acc: 0.8389\n",
      "Epoch 16/30\n",
      " - 0s - loss: 0.1558 - acc: 0.9701 - val_loss: 0.4341 - val_acc: 0.8556\n",
      "Epoch 17/30\n",
      " - 0s - loss: 0.1486 - acc: 0.9715 - val_loss: 0.4374 - val_acc: 0.8500\n",
      "Epoch 18/30\n",
      " - 0s - loss: 0.1416 - acc: 0.9694 - val_loss: 0.4147 - val_acc: 0.8611\n",
      "Epoch 19/30\n",
      " - 0s - loss: 0.1353 - acc: 0.9749 - val_loss: 0.4211 - val_acc: 0.8556\n",
      "Epoch 20/30\n",
      " - 0s - loss: 0.1290 - acc: 0.9729 - val_loss: 0.4179 - val_acc: 0.8556\n",
      "Epoch 21/30\n",
      " - 0s - loss: 0.1240 - acc: 0.9756 - val_loss: 0.4522 - val_acc: 0.8556\n",
      "Epoch 22/30\n",
      " - 0s - loss: 0.1209 - acc: 0.9749 - val_loss: 0.4137 - val_acc: 0.8556\n",
      "Epoch 23/30\n",
      " - 0s - loss: 0.1138 - acc: 0.9763 - val_loss: 0.4263 - val_acc: 0.8500\n",
      "Epoch 24/30\n",
      " - 0s - loss: 0.1106 - acc: 0.9743 - val_loss: 0.4127 - val_acc: 0.8500\n",
      "Epoch 25/30\n",
      " - 0s - loss: 0.1079 - acc: 0.9756 - val_loss: 0.3933 - val_acc: 0.8611\n",
      "Epoch 26/30\n",
      " - 0s - loss: 0.1036 - acc: 0.9777 - val_loss: 0.4238 - val_acc: 0.8667\n",
      "Epoch 27/30\n",
      " - 0s - loss: 0.1006 - acc: 0.9812 - val_loss: 0.3916 - val_acc: 0.8722\n",
      "Epoch 28/30\n",
      " - 0s - loss: 0.0987 - acc: 0.9805 - val_loss: 0.3783 - val_acc: 0.8722\n",
      "Epoch 29/30\n",
      " - 0s - loss: 0.0952 - acc: 0.9777 - val_loss: 0.3878 - val_acc: 0.8444\n",
      "Epoch 30/30\n",
      " - 0s - loss: 0.0937 - acc: 0.9784 - val_loss: 0.3848 - val_acc: 0.8667\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_Ker.fit(data_train, labels_train_cat, validation_data=(data_valid, labels_valid_cat), \n",
    "                                            epochs=30, batch_size=100, verbose=2)\n",
    "\n",
    "# Comparisong with prediction\n",
    "predicted = model_Ker.predict(data_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for <keras.engine.sequential.Sequential object at 0x12667cd68>:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.84      0.91        19\n",
      "          1       0.75      0.53      0.62        17\n",
      "          2       1.00      1.00      1.00        18\n",
      "          3       0.94      0.89      0.92        19\n",
      "          4       0.94      0.94      0.94        17\n",
      "          5       0.86      1.00      0.93        19\n",
      "          6       0.90      0.95      0.92        19\n",
      "          7       0.94      0.88      0.91        17\n",
      "          8       0.73      0.69      0.71        16\n",
      "          9       0.65      0.89      0.76        19\n",
      "\n",
      "avg / total       0.87      0.87      0.86       180\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reverse one hot encoding\n",
    "pred = uniques[predicted.argmax(1)]\n",
    "\n",
    "print(\"Classification report for %s:\\n%s\\n\"\n",
    "      % (model_Ker, metrics.classification_report(labels_valid, pred)))\n",
    "# print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(labels_valid, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/\n",
    "\n",
    "## 5) Using Tensorflow\n",
    "https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/neural_network_raw.ipynb\n",
    "\n",
    "https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/neural_network.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
